{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to be able to classify input images as HDR or SDR. To these ends, we employ a multitude of methods of doing so, from PCA analysis to Neural Networks. To add depth to our investigation, we also decided to involve julia set fractals generated based on the contrast of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDR Explained\n",
    "\n",
    "HDR images display greater detail and a wider range of colors compared to SDR. The primary difference is the dynamic range being the information range between dark and light regions of the image. HDR images calculate the amount of light in an image and use this information to normalize the brightness preventing some colors from being removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractals Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:12:01.298378Z",
     "start_time": "2020-04-17T08:12:01.290548Z"
    }
   },
   "source": [
    "Essentially, they are a way to generate infinite images. The basic concept is to take a complex number, perform a function on that number (Z^2+C) then use the output as the new input (known as an orbit). We can classify every complex number in the plane based on if the number converges or diverges through the iterations. It turns out if Z^2+C is ever greater than 2 the number will diverge, otherwise it converges. If the complex number converges we paint it black (or any color you want), otherwise you paint it proportional to how many iterations it made it through before its length was greater than 2. Chaotic spirals emerge on the borders essentially because, based on the fact that there is an infinite amount of numbers between any two numbers, if you take an complex number on the border inside the set and add some infinitesimal step, it’s basically impossible to tell if that neighbor will be inside or outside the set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project comes from [this website](ftp://lesc.dinfo.unifi.it/pub/Public/HDR/). It is a database of images of different subjects in both HDR and SDR that was used for a different research project but has since been released out to the public. The data was downloaded using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T05:03:36.438579Z",
     "start_time": "2020-04-16T05:03:36.434620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note that this script will not run unless the wget package is installed and the correct folder structure is present.\n",
    "import wget\n",
    "\t\t\n",
    "devices = ['A01_GioneeS55', 'A02_Huawei-P8', 'A03_Huawei-P9', 'A04_Huawei-P10', 'A05_Hwawie-MatePro10', 'A06_Huawei-Y5', 'A07_Galaxy-S7', 'A08_Galaxy-S7', 'A09_Galaxy-Note5', 'A10_Galaxy-J7', 'A11_Xiaomi5', 'A12_Huawei-RY6', 'A13_Huawei-RY6', 'A14_Xiaomi-5A', 'A15_Xiaom-3', 'A16_OnePlus-3t', 'A17_AsusZenfone-2', 'I03_iPhone7', 'I04_iPad-Air', 'I06_iPhone-5S']\n",
    "\n",
    "category = \"HDR\"\n",
    "for device in devices[-1:]:\n",
    "    number = 1\n",
    "    while True:\n",
    "        try:\n",
    "            link = 'ftp://lesc.dinfo.unifi.it/pub/Public/HDR/{}/NAT/{}_TRIPOD/{}_{}_TRIPOD_{:03}.jpg'.format(device, category, device[:3], category, number)\n",
    "            wget.download(link, out=f\"{device}/{category}-{number}.jpg\")\n",
    "            number += 1\n",
    "        except Exception as thing:\n",
    "            print(f\"Exception Occurred at : {category}-{number}\")\n",
    "            if category == \"HDR\":\n",
    "                category = \"SDR\"\n",
    "                number = 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is an example of an HDR image and its SDR counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T02:22:47.238415Z",
     "start_time": "2020-04-17T02:22:44.611446Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_datas = [Image.open(\"A01_GioneeS55\\HDR-1.jpg\"), Image.open(\"A01_GioneeS55\\SDR-1.jpg\")]\n",
    "\n",
    "f, axarr = plt.subplots(1,2, figsize = (15, 20))\n",
    "axarr[0].imshow(image_datas[0])\n",
    "axarr[0].axis('off')\n",
    "axarr[0].set_title('A01_GioneeS55\\HDR-1.jpg')\n",
    "axarr[1].imshow(image_datas[1])\n",
    "axarr[1].axis('off')\n",
    "axarr[1].set_title('A01_GioneeS55\\SDR-1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to formulate an algorithm to generate fractals out of all of the downloaded images. The first step towards these ends was identifying a paramter to base the fractals on and formulate a way to measure said parameter. The results of this endeavor are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:00:44.253808Z",
     "start_time": "2020-04-17T08:00:44.241818Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Print iterations progress\n",
    "# Taken from https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/9733288/how-to-programmatically-calculate-the-contrast-ratio-between-two-colors\n",
    "# Translated into python by ourselves.\n",
    "def luminanace(r, g, b):\n",
    "    a = [r, g, b]\n",
    "    for i in range(len(a)):\n",
    "        color = a[i]\n",
    "        color /= 255\n",
    "        if color <= 0.03928:\n",
    "            a[i] = color / 12.92\n",
    "        else:\n",
    "            a[i] = ((color + 0.055) / 1.055)**2.4\n",
    "    \n",
    "    return a[0] * 0.2126 + a[1] * 0.7152 + a[2] * 0.0722\n",
    "\n",
    "def contrast(rgb1, rgb2): \n",
    "    result = (luminanace(rgb1[0], rgb1[1], rgb1[2]) + 0.05) / (luminanace(rgb2[0], rgb2[1], rgb2[2]) + 0.05)\n",
    "    \n",
    "    # Ensuring the order of parameters does not change the result.\n",
    "    if result < 1: \n",
    "        result = 1/result\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_average_contrast_with_other_pixels(im, pixel):\n",
    "    contrasts = []\n",
    "    for row in range(im.shape[0]): \n",
    "        for column in range(im.shape[1]):\n",
    "            contrasts.append(contrast(pixel, im[row][column]))\n",
    "    return np.mean(contrasts)\n",
    "\n",
    "\n",
    "# This is the function that calculates the average contrast between the pixels of an image.\n",
    "# Essentially, the function iterates through every pixel in the image and for each pixel, calculates\n",
    "# the contrast between it and every other pixel in the image. The latter step is encapsulated in the above\n",
    "# function which makes use of the contrast function, which in turn makes use of the functions for calculating \n",
    "# contrast and luminance. \n",
    "def image_contrast(path):\n",
    "    im = np.asarray(Image.open(path).resize((30, 30)))\n",
    "    avg_contrasts = []\n",
    "    for row in range(im.shape[0]): \n",
    "        for column in range(im.shape[1]):\n",
    "            avg_contrasts.append(calculate_average_contrast_with_other_pixels(im, im[row][column]))\n",
    "            printProgressBar(row*im.shape[0]+column+1, im.shape[0]*im.shape[1])\n",
    "    return np.mean(avg_contrasts), np.var(avg_contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally using the above, we could generate fractals using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:16:28.430027Z",
     "start_time": "2020-04-17T08:16:28.420009Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from contrast_measurer import image_contrast\n",
    "import json\n",
    " \n",
    "\n",
    "# Function that generates julia set fractal given initial parameters.\n",
    "# Adapted from: https://tomroelandts.com/articles/how-to-compute-colorful-fractals-using-numpy-and-matplotlib\n",
    "def julia(output, m = 480, n = 320, s = 300, a = -0.4, b = 0.5, grayscale = False):\n",
    "    x = np.linspace(-m / s, m / s, num=m).reshape((1, m))\n",
    "    y = np.linspace(-n / s, n / s, num=n).reshape((n, 1))\n",
    "    Z = np.tile(x, (n, 1)) + 1j * np.tile(y, (1, m))\n",
    "    \n",
    "    C = np.full((n, m), complex(a, b))\n",
    "    M = np.full((n, m), True, dtype=bool)\n",
    "    N = np.zeros((n, m))\n",
    "    for i in range(256):\n",
    "        Z[M] = Z[M] * Z[M] + C[M]\n",
    "        M[np.abs(Z) > 2] = False\n",
    "        N[M] = i\n",
    "    \n",
    "    if grayscale:\n",
    "        # imageio.imwrite(output, np.flipud(1 - M))\n",
    "        # This is the grayscale version.\n",
    "        imageio.imwrite(output, np.flipud(255 - N))\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        # Save with Matplotlib using a colormap.\n",
    "        # This is the color version\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(m / 100, n / 100)\n",
    "        ax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(np.flipud(N))\n",
    "        plt.savefig(f'.{output}')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T02:32:59.688482Z",
     "start_time": "2020-04-17T02:32:57.187248Z"
    }
   },
   "outputs": [],
   "source": [
    "image_datas = [Image.open(\"A01_GioneeS55\\HDR-2.jpg\"), Image.open(\"A01_GioneeS55\\Fractal-HDR-2.jpg\"), Image.open(\"A01_GioneeS55\\SDR-2.jpg\"), Image.open(\"A01_GioneeS55\\Fractal-SDR-2.jpg\")]\n",
    "\n",
    "f, axarr = plt.subplots(2,2, figsize = (8, 12))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axarr[i, j].imshow(image_datas[i*2+j])\n",
    "        axarr[i, j].axis('off')\n",
    "        axarr[i, j].set_title(image_datas[i*2+j].filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this done, we proceeded to use the data we generated to attempt to train machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Here I used the same parameter grid as the one we used in class, and simply had to reassign which vectors we were using as the feature inputs. I believe it would be of great value to test these algorithms under different parameters if time would have allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation\n",
    "\n",
    "Important Note: This code is dependent upon how you imported the data into your machine. For time purposes, I have only showed the training for a single phone. If you wish to run this code, the phone image files 'A01_GioneeS55' must be saved in the same directoy as where the notebook is launched from. This shouldn't be an issue if you cloned the Github repository in which my partner did an outstanding job of collecting and organizing the data, as well as implementing the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import imageio\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(folder):\n",
    "    return glob.glob(folder+\"/*.j*\")\n",
    "\n",
    "phone = 'A01_GioneeS55'\n",
    "pictures = get_images(phone)\n",
    "fractals = pictures[0:33]\n",
    "images = pictures[33:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractal = imread(fractals[3])\n",
    "print(fractal.shape)\n",
    "plt.matshow(fractal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imread(images[3])\n",
    "print(image.shape)\n",
    "plt.matshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resize the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    " \n",
    "img = cv2.imread(images[3], cv2.IMREAD_UNCHANGED)\n",
    " \n",
    "print('Original Dimensions : ',img.shape)\n",
    " \n",
    "width = 480\n",
    "height = 320\n",
    "dim = (width, height)\n",
    "# resize image\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    " \n",
    "print('Resized Dimensions : ',resized.shape)\n",
    " \n",
    "plt.matshow(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractal_matrices = np.array([])\n",
    "for i in range(len(fractals)):\n",
    "    fractal = imread(fractals[i]).flatten()\n",
    "    fractal_matrices = np.append(fractal_matrices, fractal, axis = 0)\n",
    "fractal_matrices = fractal_matrices.reshape(len(fractals), len(fractal))\n",
    "\n",
    "image_matrices = np.array([])\n",
    "for i in range(len(images)):\n",
    "    image = cv2.imread(images[i], cv2.IMREAD_UNCHANGED)\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    image = resized.flatten()\n",
    "    image_matrices = np.append(image_matrices, image, axis = 0)\n",
    "image_matrices = image_matrices.reshape(len(images), len(image))\n",
    "\n",
    "class_f_labels = np.zeros(len(fractals))\n",
    "for i in range(len(fractals)):\n",
    "    if fractals[i][len(phone)+9:len(phone)+12] == 'HDR':\n",
    "        class_f_labels[i] = 1\n",
    "    else:\n",
    "        class_f_labels[i] = 0\n",
    "\n",
    "class_i_labels = np.zeros(len(images))\n",
    "for i in range(len(images)):\n",
    "    if images[i][len(phone)+1:len(phone)+4] == 'HDR':\n",
    "        class_i_labels[i] = 1\n",
    "    else:\n",
    "        class_i_labels[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fractal_matrices.shape)\n",
    "print(image_matrices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "train_f_vectors, test_f_vectors, train_f_labels, test_f_labels = model_selection.train_test_split(fractal_matrices, class_f_labels, train_size=0.75,test_size=0.25)\n",
    "\n",
    "train_i_vectors, test_i_vectors, train_i_labels, test_i_labels = model_selection.train_test_split(image_matrices, class_i_labels, train_size=0.75,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractal SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "tmp_vectors = train_f_vectors\n",
    "tmp_labels = train_f_labels\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Runtime\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vectors = test_f_vectors\n",
    "true_lables = test_f_labels\n",
    "\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_lables, pred_labels))\n",
    "print(confusion_matrix(true_lables, pred_labels, labels=range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "tmp_vectors = train_i_vectors\n",
    "tmp_labels = train_i_labels\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Runtime\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vectors = test_i_vectors\n",
    "true_lables = test_i_labels\n",
    "\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_lables, pred_labels))\n",
    "print(confusion_matrix(true_lables, pred_labels, labels=range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractal PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = len(train_f_vectors)\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "\n",
    "pca_fit = pca.fit(train_f_vectors)\n",
    "\n",
    "pca_train_f_vectors = pca.transform(train_f_vectors)\n",
    "pca_test_f_vectors = pca.transform(test_f_vectors)\n",
    "\n",
    "print(\"Training set changed from a size of: \", train_f_vectors.shape, ' to: ', pca_train_f_vectors.shape)\n",
    "print(\"Testing set changed from a size of: \", test_f_vectors.shape, ' to: ', pca_test_f_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_vectors = pca_train_f_vectors\n",
    "tmp_labels = train_f_labels\n",
    "\n",
    "rerun_training = True\n",
    "\n",
    "start = time.time()\n",
    "if rerun_training:\n",
    "    \n",
    "    print(\"Fitting the classifier to the training set\")\n",
    "    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "    clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Runtime\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vectors = pca_test_f_vectors\n",
    "true_labels = test_f_labels\n",
    "\n",
    "print(\"Predicting SDR vs. HDR for fractal inputs\")\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(confusion_matrix(true_labels, pred_labels, labels=range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = len(train_i_vectors)\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "\n",
    "pca_fit = pca.fit(train_i_vectors)\n",
    "\n",
    "pca_train_i_vectors = pca.transform(train_i_vectors)\n",
    "pca_test_i_vectors = pca.transform(test_i_vectors)\n",
    "\n",
    "print(\"Training set changed from a size of: \", train_i_vectors.shape, ' to: ', pca_train_i_vectors.shape)\n",
    "print(\"Testing set changed from a size of: \", test_i_vectors.shape, ' to: ', pca_test_i_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_vectors = pca_train_i_vectors\n",
    "tmp_labels = train_i_labels\n",
    "\n",
    "rerun_training = True\n",
    "\n",
    "start = time.time()\n",
    "if rerun_training:\n",
    "    \n",
    "    print(\"Fitting the classifier to the training set\")\n",
    "    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "    clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "    print(\"Best estimator found by grid search:\")\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Runtime\",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vectors = pca_test_vectors\n",
    "true_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting SDR vs. HDR for fractal inputs\")\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(confusion_matrix(true_labels, pred_labels, labels=range(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision to investigate neural networks as a potential solution was due to their dominant prevalence in difficult to algorithmically define problems such as these. Our investigation led to us using a specific type of neural network - the convolutional neural network (CNN) - due to its particular effectiveness in image classfication problems. CNNs are similar to basic neural networks, with the difference lying in the use of filters as parts of the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning \n",
    "Code adapted from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#load-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even within CNNs, there were multiple ways to go about the process. One such method was transfer learning. This method essentially involves using a model that has already been pre trained on a large dataset and 'repurposing' it by training it\n",
    "on the new data. Doing so is supposed to enhance effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Fractals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T05:40:56.118869Z",
     "start_time": "2020-04-16T05:40:56.113884Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "torch.multiprocessing.freeze_support()\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data__dir'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                        data_transforms[x])\n",
    "                for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "            for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.savefig(\"thing.jpg\")  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "        \n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=24)\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:35:40.858952Z",
     "start_time": "2020-04-17T03:35:40.855950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logs for the last few epochs\n",
    "# Epoch 20/23\n",
    "# ----------\n",
    "# train Loss: 0.6365 Acc: 0.6323\n",
    "# val Loss: 0.8214 Acc: 0.4528\n",
    "\n",
    "# Epoch 21/23\n",
    "# ----------\n",
    "# train Loss: 0.6503 Acc: 0.6355\n",
    "# val Loss: 0.8193 Acc: 0.4245\n",
    "\n",
    "# Epoch 22/23\n",
    "# ----------\n",
    "# train Loss: 0.6511 Acc: 0.6226\n",
    "# val Loss: 0.8228 Acc: 0.3962\n",
    "\n",
    "# Epoch 23/23\n",
    "# ----------\n",
    "# train Loss: 0.6693 Acc: 0.5871\n",
    "# val Loss: 0.8198 Acc: 0.4151\n",
    "\n",
    "# Training complete in 7m 4s\n",
    "# Best val Acc: 0.500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model had accuracy exactly 50%. The following is some of the classifications made by the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transferlearningwithout.jpg\" alt=\"Drawing\" style=\"width: 640px; height:480px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T04:38:20.189253Z",
     "start_time": "2020-04-16T04:38:20.178283Z"
    }
   },
   "source": [
    "#### With Fractals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T05:42:40.818443Z",
     "start_time": "2020-04-16T05:42:37.758627Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'data__dir_fractal'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                        data_transforms[x])\n",
    "                for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "            for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=24)\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:39:49.867514Z",
     "start_time": "2020-04-17T03:39:49.863493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logs for the last few epochs\n",
    "# Epoch 20/23\n",
    "# ----------\n",
    "# train Loss: 0.6909 Acc: 0.5629\n",
    "# val Loss: 0.7062 Acc: 0.5175\n",
    "\n",
    "# Epoch 21/23\n",
    "# ----------\n",
    "# train Loss: 0.6734 Acc: 0.5695\n",
    "# val Loss: 0.7168 Acc: 0.5000\n",
    "\n",
    "# Epoch 22/23\n",
    "# ----------\n",
    "# train Loss: 0.6773 Acc: 0.5795\n",
    "# val Loss: 0.7143 Acc: 0.5000\n",
    "\n",
    "# Epoch 23/23\n",
    "# ----------\n",
    "# train Loss: 0.6881 Acc: 0.5695\n",
    "# val Loss: 0.7289 Acc: 0.5263\n",
    "\n",
    "# Training complete in 2m 56s\n",
    "# Best val Acc: 0.561404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the model had a slightly better accuracy of 56%. The following are the results for the model trained with the fractals instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transferlearningwith.jpg\" alt=\"Drawing\" style=\"width: 640px; height:480px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T17:29:25.137189Z",
     "start_time": "2020-04-15T17:29:25.013328Z"
    }
   },
   "source": [
    "### Binary Classifier\n",
    "Code for this section adapted from: https://github.com/jayrodge/Binary-Image-Classifier-PyTorch/blob/master/Binary_face_classifier.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T05:43:28.738377Z",
     "start_time": "2020-04-16T05:43:28.734353Z"
    }
   },
   "source": [
    "The second method of using CNNs was to not use transfer learning and create the network from scratch using the available data. Since our data is of only two classes, we decided to implement a binary classification CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T04:39:03.155883Z",
     "start_time": "2020-04-16T04:39:03.151877Z"
    }
   },
   "source": [
    "#### Without Fractals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "# percentage of training set to use as validation\n",
    "test_size = 0.3\n",
    "valid_size = 0.1\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "data = datasets.ImageFolder('data__dir_binary',transform=transform)\n",
    "\n",
    "#For test\n",
    "num_data = len(data)\n",
    "indices_data = list(range(num_data))\n",
    "np.random.shuffle(indices_data)\n",
    "split_tt = int(np.floor(test_size * num_data))\n",
    "train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt]\n",
    "\n",
    "#For Valid\n",
    "num_train = len(train_idx)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(data, sampler = test_sampler, batch_size=batch_size, \n",
    "    num_workers=1)\n",
    "classes = [0,1]\n",
    "\n",
    "for batch in valid_loader:\n",
    "    print(batch[0].size())\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "    plt.savefig(\"thing.jpg\")\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32*53*53, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.003, momentum= 0.9)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 5 # you may increase this number to train a final model\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    # track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "model.eval()\n",
    "i=1\n",
    "# iterate over test data\n",
    "len(test_loader)\n",
    "for data, target in test_loader:\n",
    "    i=i+1\n",
    "    if len(target)!=batch_size:\n",
    "        continue\n",
    "\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "#     print(target)\n",
    "\n",
    "    for i in range(batch_size):       \n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(2):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:33:46.259760Z",
     "start_time": "2020-04-17T03:33:46.255769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Epoch: 1        Training Loss: 0.421299         Validation Loss: 0.041617\n",
    "# Validation loss decreased (inf --> 0.041617).  Saving model ...\n",
    "# Epoch: 2        Training Loss: 0.371931         Validation Loss: 0.037014\n",
    "# Validation loss decreased (0.041617 --> 0.037014).  Saving model ...\n",
    "# Epoch: 3        Training Loss: 0.369603         Validation Loss: 0.038283\n",
    "# Epoch: 4        Training Loss: 0.359903         Validation Loss: 0.037351\n",
    "# Epoch: 5        Training Loss: 0.349221         Validation Loss: 0.037590\n",
    "# Test Loss: 0.195213\n",
    "\n",
    "# Test Accuracy (Overall): 50% (48/96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model without fractals was 50%. Some of the classifications are shown below. 1 means that the image was classified as HDR and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:10:41.854237Z",
     "start_time": "2020-04-17T03:10:41.849175Z"
    }
   },
   "source": [
    "<img src=\"thing.jpg\" alt=\"Drawing\" style=\"width: 1000px; height:400px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T04:38:51.798402Z",
     "start_time": "2020-04-16T04:38:51.793429Z"
    }
   },
   "source": [
    "#### With Fractals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "# percentage of training set to use as validation\n",
    "test_size = 0.3\n",
    "valid_size = 0.1\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "data = datasets.ImageFolder('data__dir_binary_fractal',transform=transform)\n",
    "\n",
    "#For test\n",
    "num_data = len(data)\n",
    "indices_data = list(range(num_data))\n",
    "np.random.shuffle(indices_data)\n",
    "split_tt = int(np.floor(test_size * num_data))\n",
    "train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt]\n",
    "\n",
    "#For Valid\n",
    "num_train = len(train_idx)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(data, sampler = test_sampler, batch_size=batch_size, \n",
    "    num_workers=1)\n",
    "classes = [0,1]\n",
    "\n",
    "for batch in valid_loader:\n",
    "    print(batch[0].size())\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "    plt.savefig(\"thing.jpg\")\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(10):\n",
    "    ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32*53*53, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.003, momentum= 0.9)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 5 # you may increase this number to train a final model\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    # track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "model.eval()\n",
    "i=1\n",
    "# iterate over test data\n",
    "len(test_loader)\n",
    "for data, target in test_loader:\n",
    "    i=i+1\n",
    "    if len(target)!=batch_size:\n",
    "        continue\n",
    "\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "#     print(target)\n",
    "\n",
    "    for i in range(batch_size):       \n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(2):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:33:54.779891Z",
     "start_time": "2020-04-17T03:33:54.775880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Epoch: 1        Training Loss: 0.401943         Validation Loss: 0.039427\n",
    "# Validation loss decreased (inf --> 0.039427).  Saving model ...\n",
    "# Epoch: 2        Training Loss: 0.392773         Validation Loss: 0.038636\n",
    "# Validation loss decreased (0.039427 --> 0.038636).  Saving model ...\n",
    "# Epoch: 3        Training Loss: 0.383426         Validation Loss: 0.041589\n",
    "# Epoch: 4        Training Loss: 0.386041         Validation Loss: 0.039223\n",
    "# Epoch: 5        Training Loss: 0.381064         Validation Loss: 0.038848\n",
    "# Test Loss: 0.177392\n",
    "\n",
    "# Test Accuracy (Overall): 53% (51/96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model with fractals was 53%. Here are some of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:11:27.251574Z",
     "start_time": "2020-04-17T03:11:27.247552Z"
    }
   },
   "source": [
    "<img src=\"thingwith.jpg\" alt=\"Drawing\" style=\"width: 1000px; height:400px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T04:40:22.162915Z",
     "start_time": "2020-04-16T04:40:21.944499Z"
    }
   },
   "source": [
    "## Luminance Subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we wanted to test one more method that was a much simpler problem statement. Essentially, given two images, if it is known that one of them is HDR and the other SDR, is it possible to predict which is which? To these ends, we surmised an algorithm that computes the a property we term as the luminance difference. Essentially, we iterated through all the images of both pixels simultaneously and for each pixel in one image, we computed the difference between its luminance and the luminance of the pixel in the same position in the other image. The function that does so is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:02:34.991494Z",
     "start_time": "2020-04-17T08:02:34.985451Z"
    }
   },
   "outputs": [],
   "source": [
    "def luminance_diff(path1, path2):\n",
    "    im1 = np.asarray(Image.open(path1).resize((30, 30)))\n",
    "    im2 = np.asarray(Image.open(path2).resize((30, 30)))\n",
    "    contrasts = []\n",
    "    for row in range(im1.shape[0]): \n",
    "        for column in range(im1.shape[1]):\n",
    "            c1 = im1[row][column]\n",
    "            c2 = im2[row][column]\n",
    "            contrasts.append(luminanace(c1[0], c1[1], c1[2])-luminanace(c2[0], c2[1], c2[2]))\n",
    "            printProgressBar(row*im1.shape[0]+column+1, im1.shape[0]*im1.shape[1])\n",
    "    return np.mean(contrasts), np.var(contrasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of using the above function on our data is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:52:19.973155Z",
     "start_time": "2020-04-17T07:52:19.878636Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "neg_count = 0\n",
    "pos_count = 0\n",
    "data = json.load(open(\"luminance_data.json\"))\n",
    "for item in data:\n",
    "    for entry in data[item]:\n",
    "        name, luminance_data = entry\n",
    "        diff = luminance_data[0]\n",
    "        if diff < 0:\n",
    "            pos_count += 1\n",
    "        else:\n",
    "            neg_count += 1\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "labels = [f'HDR less than SDR ({pos_count})', f'SDR less than HDR ({neg_count})']\n",
    "counts = [pos_count, neg_count]\n",
    "plt.bar(labels,counts)\n",
    "plt.title(\"Comparison of Luminance Difference\")\n",
    "percentage = pos_count/(pos_count+neg_count)*100\n",
    "print(f'Thus, if one were to use the luminance_diff value of one of the images \\nbeing lower as a classifier, {percentage:.2f}% accuracy could be obtained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our investigation did not lead to any particular grand final result, however, this is not to say that the project was fruitless, and there is still much left to be said about the nature of the problem. In comparing methods, it seemed the luminance subtraction method, which needed to be given exactly two images of the same object but different HDR/SDR classes, performed the best though even this can hardly be called better than any of the other methods used. One interesting fact (possibly a coincidence) is that the CNN method using fractals was able to achieve accuracy of 56.14% in spite of the fact that it did not need any special conditions like the luminance_diff algorithm. Unfortunately, we did not have the technical expertise or time necessary to completely investigate as to why this was.\n",
    "\n",
    "As far as things we would do differently next time, we would likely attempt to obtain more data and attempt our different methods once again while fine-tuning parameters. The problem may be greatly simplified by first converting both the images and fractals to grayscale since it decreases the number of degrees of freedom and shrinks the size of our input matrix by a factor of 3x. The problem tends to grow exponentially since for example a 10 x 10 RGB image has 300 pieces of information; if you then wanted to increase the size of the image to 100 x 100 (Note: this doesn't necessarily mean a wider or larger image but a more detailed image) this RGB image now has 30,000 pieces of information. Contrary to increasing a 10 x 10 grayscale image to a 100 x 100 image which has 10,000 data points, and we are able to increase the clarity of our image by 10x with 3x less cost. Another aspect of the problem we would need to spend more time on is choosing which features are most important for this type of image classification. We picked a rather vague concept for image classification being HDR vs. SDR. It's not like we are comparing cats to dogs where the distinctions are often very obvious to humans and stored in the 'physical' structure of the image while when comparing HDR to SDR the physical disinctions are much more subtle. Furthermore, we would also wish to experiment with other types of neural networks and models which could not be done due to the time constraints surrounding this project. \n",
    "\n",
    "Ultimately, we are satisfied with what we were able to accomplish given the circumstances, and we have learned a great deal about fractals, creating and using neural networks (being an industry standard tool), and image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
